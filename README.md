# SVERL_improvement
Улучшение подхода SVERL для объяснения поведения RL агента с помощью векторов Шепли.

Часть кода взята из статьи с ICML-2023. 
Ее название Explaining Reinforcement Learning with Shapley Values. Авторы: Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek.

Ссылка на их репозиторий: https://github.com/djeb20/SVERL_icml_2023

В этом репозитории я представляю проверку части их кода, предлагаю свой подход к подсчету измененных векторов Шепли для объяснения RL и провожу эксперименты.
Основной код находится в ноутбуке main.ipynb. 

Инструкция к запуску: в окружение к ноутбуку следует положить q_agent_1.py, qwc.py, utils.py, characteristics.py. Затем достаточно запустить код в google colab 
или в другой удобной среде. 

Необходимые комментарии также написаны в ноутбуке. 
